{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code :  Mine tweet details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tweepy to mine tweet details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json_lines\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert keys to access the twitter api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"4839506722-SkV6JbXgYKw4xx7scZPBxviFattLdy55clZQiHV\"\n",
    "access_token_secret = \"wZ829cwP8HDARYuJa7zdfquVTU7tzFh0CMdNqBbK0k30Z\"\n",
    "consumer_key = \"6DGxRmh6e0dgSfIpPDd5o3nKp\"\n",
    "consumer_secret = \"Vppd7vE7TBOtuygktcTX2ZcXwywcKZbVc6O7mQXjk5E1zYTKFI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet reads in the tweets, prints the total number for the given user ID and stores the tweets to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 336\n",
      "Json file saved!\n"
     ]
    }
   ],
   "source": [
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print (\"Data is:\", data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print (\"Status is:\", status)\n",
    "\n",
    "\n",
    "def get_all_tweets(screen_name):    \n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    auth_api = API(auth)\n",
    "    \n",
    "    all_tweets = []\n",
    "    \n",
    "    #First 200 tweets\n",
    "    new_tweets = auth_api.user_timeline(screen_name=screen_name, count=200)\n",
    "    all_tweets.extend(new_tweets)\n",
    "    \n",
    "    oldest_tweet = all_tweets[-1].id - 1\n",
    "    \n",
    "    #rest of the tweets if any!\n",
    "    while len(new_tweets) :\n",
    "        new_tweets = auth_api.user_timeline(screen_name=screen_name, count=200, max_id=oldest_tweet)\n",
    "        all_tweets.extend(new_tweets)\n",
    "        oldest_tweet = all_tweets[-1].id - 1      \n",
    "           \n",
    "    #print the number of tweets for the given user id\n",
    "    print(\"Number of tweets:\", len(all_tweets))\n",
    "    \n",
    "    #dump into json file\n",
    "    with open('output.jsonl', 'w', encoding='utf8') as outfile:\n",
    "        for entry in all_tweets:\n",
    "            json.dump(entry._json, outfile)\n",
    "            outfile.write('\\n')           \n",
    "            \n",
    "    print(\"Json file saved!\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    # Enter the twitter handle of the user id concerned\n",
    "    get_all_tweets(\"midasIIITD\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet reads the json file with the tweets and displays the required details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text  \\\n",
      "0    @himanchalchandr Meanwhile, complete CV/NLP ta...   \n",
      "1    @sayangdipto123 Submit as per the guideline ag...   \n",
      "2    We request all students whose interview are sc...   \n",
      "3    Other queries: \"none of the Tweeter Apis give ...   \n",
      "4    Other queries: \"do we have to make two differe...   \n",
      "5    Other queries: \"If using Twitter api, it does ...   \n",
      "6    Response to some queries asked by students on ...   \n",
      "7    RT @kdnuggets: Top 8 #Free Must-Read #Books on...   \n",
      "8    @nupur_baghel @PennDATS Congratulation @nupur_...   \n",
      "9    We have emailed the task details to all candid...   \n",
      "10   RT @rfpvjr: Our NAACL paper on polarization in...   \n",
      "11   RT @kdnuggets: Effective Transfer Learning For...   \n",
      "12   RT @stanfordnlp: Whatâ€™s new in @Stanford CS224...   \n",
      "13   RT @DeepMindAI: Today we're releasing a large-...   \n",
      "14   RT @ylecun: Congratulations Jitendra Malik !\\n...   \n",
      "15   RT @IIITDelhi: Another chance to take admissio...   \n",
      "16   Dear @midasIIITD internship candidates who hav...   \n",
      "17   Looking forward to your paper submission to @I...   \n",
      "18   RT @ngrams: Reproducibility in multimedia rese...   \n",
      "19   Online application for https://t.co/DJFDrQsHZP...   \n",
      "20   RT @ACMMM19: A final reminder of the Reproduci...   \n",
      "21   RT @isarth23: Thanks for the support and help ...   \n",
      "22   Since SemEval-2019 will be held June 6-7, 2019...   \n",
      "23   +@aggarwal_kartik.\\nCongrats! Wish you many mo...   \n",
      "24   RT @aggarwal_kartik: Our work (@midasIIITD ) a...   \n",
      "25   Congratulations! @midasIIITD team, @isarth23 @...   \n",
      "26   @EEMLcommunity @radamihalcea too many deadline...   \n",
      "27   RT @stanfordnlp: CS224N Natural Language Proce...   \n",
      "28   RT @ylecun: Learn PyTorch by running on Google...   \n",
      "29   Dr. Vineeth N Balasubramanian will present a T...   \n",
      "..                                                 ...   \n",
      "306  RT @TensorFlow: TensorFlow 1.10.0 has been rel...   \n",
      "307  @midasIIITD is looking for motivated IIITD MTe...   \n",
      "308  @IIITDelhi @ponguru @RatnRajiv The results of ...   \n",
      "309  RT @IIITDelhi: @midasIIITD has secured rank 1 ...   \n",
      "310  RT @kdnuggets: Comparison of Top 6 Python NLP ...   \n",
      "311  Check more details of the 20th IEEE Internatio...   \n",
      "312  MR2AMC@ISM 2018 will be organized by @RatnRaji...   \n",
      "313  Our workshop proposal named, \"MR2AMC: Multimod...   \n",
      "314  @NUSComputing Congratulations Abdelhak and Pro...   \n",
      "315  RT @goodfellow_ian: One of the most anticipate...   \n",
      "316     @the_dhumketu Great to have you in @midasIIITD   \n",
      "317  Congratulation @soujanyaporia for being appoin...   \n",
      "318  @IIITDelhi @the_dhumketu Thanks team @midasIII...   \n",
      "319  RT @IIITDelhi: Congratulations @midasIIITD int...   \n",
      "320  RT @learning_pt: Profile of the 5 Indian under...   \n",
      "321  Have a look at the list of accepted papers in ...   \n",
      "322  RT @goodfellow_ian: https://t.co/hYiWI7ntyk Te...   \n",
      "323  RT @IIITDelhi: Congratulations Dr. @RatnRajiv ...   \n",
      "324  RT @ylecun: Jitendra Malik, who directs FAIR-M...   \n",
      "325  RT @kdnuggets: .@Bloomberg launches free cours...   \n",
      "326  RT @TechAtBloomberg: Missed #PyLondinium18? Wa...   \n",
      "327  RT @IIITDelhi: We are delighted to announce th...   \n",
      "328  Get ready for the annual technical fest of @II...   \n",
      "329  Congratulations Dr. @RatnRajiv and team @midas...   \n",
      "330  Congratulations MIDAS @midasIIITD intern Prakh...   \n",
      "331    MIDAS@IIITD foundation. https://t.co/LKuzyBHzjm   \n",
      "332  It feels great to be the part of @IIITDelhi. h...   \n",
      "333  Thank you, @toonzratn for designing the logo o...   \n",
      "334  We are on Facebook too. Like our page to get o...   \n",
      "335  MIDAS is a group of researchers at IIIT-Delhi ...   \n",
      "\n",
      "                      Date and Time Number of Favorites Number of re-tweets  \\\n",
      "0    Sun Apr 07 14:17:29 +0000 2019                   0                   0   \n",
      "1    Sun Apr 07 14:17:09 +0000 2019                   0                   0   \n",
      "2    Sun Apr 07 11:43:24 +0000 2019                   0                   1   \n",
      "3    Sun Apr 07 06:55:19 +0000 2019                   5                   2   \n",
      "4    Sun Apr 07 06:53:38 +0000 2019                   4                   1   \n",
      "5    Sun Apr 07 05:32:27 +0000 2019                   5                   1   \n",
      "6    Sun Apr 07 05:29:40 +0000 2019                   7                   1   \n",
      "7    Sat Apr 06 17:11:29 +0000 2019                   0                   2   \n",
      "8    Sat Apr 06 16:43:27 +0000 2019                  17                   3   \n",
      "9    Fri Apr 05 16:08:37 +0000 2019                  11                   1   \n",
      "10   Fri Apr 05 04:05:11 +0000 2019                   0                  16   \n",
      "11   Fri Apr 05 04:04:43 +0000 2019                   0                  11   \n",
      "12   Wed Apr 03 18:31:53 +0000 2019                   0                  59   \n",
      "13   Wed Apr 03 17:04:32 +0000 2019                   0                 851   \n",
      "14   Wed Apr 03 09:03:40 +0000 2019                   0                  16   \n",
      "15   Wed Apr 03 07:46:02 +0000 2019                   0                   4   \n",
      "16   Tue Apr 02 04:20:13 +0000 2019                   8                   1   \n",
      "17   Tue Apr 02 02:44:54 +0000 2019                   5                   1   \n",
      "18   Tue Apr 02 02:35:44 +0000 2019                   0                   7   \n",
      "19   Mon Apr 01 06:53:08 +0000 2019                   7                   2   \n",
      "20   Sun Mar 31 10:21:24 +0000 2019                   0                  10   \n",
      "21   Fri Mar 29 19:43:24 +0000 2019                   0                   2   \n",
      "22   Fri Mar 29 17:16:40 +0000 2019                   9                   1   \n",
      "23   Fri Mar 29 17:04:30 +0000 2019                   2                   0   \n",
      "24   Fri Mar 29 17:03:29 +0000 2019                   0                   1   \n",
      "25   Fri Mar 29 17:02:24 +0000 2019                   9                   1   \n",
      "26   Fri Mar 29 05:35:22 +0000 2019                   0                   0   \n",
      "27   Thu Mar 28 16:55:01 +0000 2019                   0                 715   \n",
      "28   Thu Mar 28 16:54:37 +0000 2019                   0                 157   \n",
      "29   Wed Mar 27 16:09:09 +0000 2019                   4                   1   \n",
      "..                              ...                 ...                 ...   \n",
      "306  Thu Aug 09 05:59:57 +0000 2018                   0                 265   \n",
      "307  Wed Aug 08 11:30:56 +0000 2018                   2                   1   \n",
      "308  Wed Aug 08 05:53:48 +0000 2018                   3                   1   \n",
      "309  Wed Aug 08 05:45:58 +0000 2018                   0                   1   \n",
      "310  Tue Aug 07 07:16:33 +0000 2018                   0                  40   \n",
      "311  Tue Aug 07 02:05:12 +0000 2018                   1                   1   \n",
      "312  Tue Aug 07 01:58:49 +0000 2018                   1                   1   \n",
      "313  Tue Aug 07 01:50:33 +0000 2018                   1                   1   \n",
      "314  Mon Aug 06 17:48:23 +0000 2018                   0                   0   \n",
      "315  Mon Aug 06 17:46:59 +0000 2018                   0                 103   \n",
      "316  Mon Aug 06 06:06:47 +0000 2018                   0                   0   \n",
      "317  Fri Aug 03 05:56:33 +0000 2018                   6                   1   \n",
      "318  Wed Aug 01 11:47:15 +0000 2018                   5                   1   \n",
      "319  Wed Aug 01 11:20:07 +0000 2018                   0                   4   \n",
      "320  Wed Aug 01 05:06:47 +0000 2018                   0                   4   \n",
      "321  Tue Jul 31 12:11:52 +0000 2018                   1                   0   \n",
      "322  Tue Jul 31 02:06:26 +0000 2018                   0                 264   \n",
      "323  Mon Jul 30 07:30:51 +0000 2018                   0                   2   \n",
      "324  Sat Jul 28 11:07:11 +0000 2018                   0                  57   \n",
      "325  Sat Jul 28 06:14:09 +0000 2018                   0                 105   \n",
      "326  Sat Jul 28 06:13:48 +0000 2018                   0                   7   \n",
      "327  Sat Jul 28 04:08:21 +0000 2018                   0                   6   \n",
      "328  Fri Jul 27 06:46:44 +0000 2018                   3                   2   \n",
      "329  Fri Jul 27 04:07:31 +0000 2018                   8                   2   \n",
      "330  Wed Jul 25 05:14:35 +0000 2018                   5                   1   \n",
      "331  Tue Jul 24 10:33:23 +0000 2018                   2                   1   \n",
      "332  Tue Jul 24 10:12:34 +0000 2018                   2                   1   \n",
      "333  Tue Jul 24 09:46:26 +0000 2018                   4                   1   \n",
      "334  Mon Jul 23 16:25:05 +0000 2018                   3                   1   \n",
      "335  Mon Jul 23 12:53:15 +0000 2018                   7                   4   \n",
      "\n",
      "    Number of Images  \n",
      "0               None  \n",
      "1               None  \n",
      "2               None  \n",
      "3               None  \n",
      "4               None  \n",
      "5               None  \n",
      "6               None  \n",
      "7               None  \n",
      "8               None  \n",
      "9               None  \n",
      "10              None  \n",
      "11                 1  \n",
      "12              None  \n",
      "13              None  \n",
      "14              None  \n",
      "15              None  \n",
      "16              None  \n",
      "17              None  \n",
      "18              None  \n",
      "19              None  \n",
      "20              None  \n",
      "21              None  \n",
      "22              None  \n",
      "23              None  \n",
      "24              None  \n",
      "25              None  \n",
      "26              None  \n",
      "27              None  \n",
      "28              None  \n",
      "29              None  \n",
      "..               ...  \n",
      "306             None  \n",
      "307             None  \n",
      "308                1  \n",
      "309             None  \n",
      "310                1  \n",
      "311             None  \n",
      "312             None  \n",
      "313             None  \n",
      "314             None  \n",
      "315                1  \n",
      "316             None  \n",
      "317             None  \n",
      "318                1  \n",
      "319             None  \n",
      "320             None  \n",
      "321             None  \n",
      "322             None  \n",
      "323             None  \n",
      "324             None  \n",
      "325             None  \n",
      "326             None  \n",
      "327             None  \n",
      "328             None  \n",
      "329             None  \n",
      "330             None  \n",
      "331             None  \n",
      "332             None  \n",
      "333                1  \n",
      "334             None  \n",
      "335             None  \n",
      "\n",
      "[336 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Display submission details\n",
    "\n",
    "tweet_data =[]\n",
    "df=pd.DataFrame(columns=['Text','Date and Time','Number of Favorites','Number of re-tweets','Number of Images'])\n",
    "\n",
    "with open('output.jsonl', 'rb') as f:\n",
    "    for item in json_lines.reader(f):\n",
    "        image_count=0\n",
    "        media = (item['entities'].get('media',[]))\n",
    "        if (len(media)>0):\n",
    "            image_count=str(len(media))\n",
    "        else:\n",
    "            image_count = 'None'\n",
    "        \n",
    "        df = df.append({'Text':item['text'],'Date and Time':item['created_at'],\n",
    "                        'Number of Favorites':item['favorite_count'],\n",
    "                        'Number of re-tweets':item['retweet_count'],\n",
    "                        'Number of Images':image_count}, ignore_index = True)\n",
    "    print(df)\n",
    "\n",
    "               \n",
    "      \n",
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
